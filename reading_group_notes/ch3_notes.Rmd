---
title: 'Gelman & Hill Ch 3 - Linear regression: the basics'
author: "Anna Spiers"
date: "2/4/2020"
output:
  html_document: default
  pdf_document: default
---

``` {r, include=FALSE}
library(arm) #display
```

### Main take-aways

#### Terminology and notation  
* Take $Y = mX + b + error$  
    * $Y$ is the *outcome* variable (aka explanatory, response)  
    * $X$ is the *predictor* variable  
    * *Dependent* and *independent* are terms for describing probability distributions 
    * $y_i$ is an element of $Y$ and is the outcome for the $i^{th}$ individual
* Individual data points are the *units of analysis*    
* Take $kids.score = 58 + 16*mom.hs + 0.5*mom.iq - 0.2*mom.hs*mom.iq + error$  
    * Four *predictors*: intercept, $mom.hs$, $mom.iq$, interaction  
    * Two *inputs*: $mom.hs, mom.iq$  

#### One predictor
* Think of a regression as a comparison of averages rather than a representation of relationships between variables. In this mindset, we are explicit about the regression’s limitations for defining relationships causally.      
*  The points on the regression line are average outcome values for subpopulations defined by these outcome values
    * E.g., children whose mothers have an IQ of 100 will on average score 6 points higher than children whose mothers have an IQ of 90  
* The *intercept* is the average outcome value when all predictor values are 0. This is mathematically sound, but not necessarily realistically sound (e.g., IQ)    

#### Multiple predictors
* aka multiple linear regression
* Interpretation
    * Put simply, you can interpret each coefficient “with all other predictors held constant”. However, this interpretation doesn’t always make sense (e.g., with predictors IQ and IQ^2, or interactions).
    ???????????
I don’t understand the difference between conterfactual and predictive interprataions ?????????

#### Interactions
* Including interaction terms is a way to allow a model to be fit differently to different subsets of data
* Adding an interaction between two groups allows the slope to vary between their subgroups (e.g., maternal high school completion and maternal IQ)
* Some coefficients are interpretable only for certain subgroups

#### Statistical inference
* Regression in vector-matrix notation
    * The deterministic prediction of an outcome can be written out in vector-matrix notation: $X_i\beta = \beta_1X_{i1} + ... + \beta_kX_{ik}$, where
        * $\beta$ is a vector and $X$ is a matrix
            * $X$ is made of $X_i$s, and each $X_i$ is a predictor variable vector for the model
            * $\beta$ is a coefficient vector
        * Take the child score model as an example, $X_i\beta = \beta_1X_{i1} + ... + \beta_kX_{ik}$
            * there are $i$ = $1,...,n = 1378$ units of children's test scores 
            * there are $k = 4$ predictors in vector $X_i$
                * $X_{i1}$ is a constant term that is 1 for each $i$. This constant is multiplied by $\beta_1$ to make the intercept
                * $X_{i2}$ is the mother's high school completion status (0 or 1) 
                * $X_{i3}$ is the mother's IQ score
                * $X_{i4}$ is the interaction between mother's IQ score and high school completion status
            * $\beta$ has length $k=4$ too
            * See Fig 3.6
* The following are two equivalent ways of writing the model
    * Classical linear regression model
        * $y_i = X_i\beta + \epsilon_i$ written concisely
        * $y_i = \beta_1X_{i1} + ... + \beta_kX_{ik} + \epsilon_i$ for $i = 1,...,n$ where errors $\epsilon_i$ have independent normal distributions with mean 0 and sd $\sigma$
    * Distribution form
        * $y_i \sim N(X_i\beta, \sigma^2)$ for $i = 1,...,n$ where $X$ is an $n$x$k$ matrix with $i^{th}$ row $X_i$
    * Multivariate notation 
        * $y \sim N(X\beta, \sigma^2I)$, where $y$ is a vector of length $n$, $X$ is a $n$x$k$ matrix of predictors, $\beta$ is a column vector of length $k$ and $I$ is the $n$x$n$ identity matrix
* Fitting and summarizing regressions in R
    * use display() function to summarize a linear model, rather than print() (displays too little info) or summary() (displays too much info)
* Least squares estimate of the vector of regression coefficients, $\beta$
    * $\hat \beta$ is estimated by minimizing $\sum_{i=1}^{n} (y_i - X_i\hat \beta)^2$. By doing so, we minimize error in our prediction
    * the least squares estimate of $\hat \beta$ is a linear transformation of the outcomes $y$, specifically $\hat \beta = (X^tX)^{-1}X^ty$
    * bonus: the least squares estimate is also the maximum likelihood estimate if the errors are independent and with equan variance and normally distributed
* Standard error of $\hat \beta$ quantifies the uncertainty in the coefficient estimates.
    * the data are roughly consistent with values of $\beta$ in the range between the estimate +- 2 SE
    * The uncertainty in the coefficient estimates will be correlated when not using a study with a balanced design, but more on that in ch7
* residuals can be used to diagnose problems in the model
    * AIS what does this sentence mean? "As a byproduct of the least squares estimation of $\beta$, the residuals, $r_i$ will be uncorrelated with all the predictors in the model
    * The residual standard deviation, $\hat \sigma$ is a measure of the average distance each observation falls from its model prediction
    * $R^2$ is the fraction of variance "explained" but the model. The "unexplained variance is $\hat \sigma^2$
        * $R^2 = 1-\hat \sigma^2/s_y^2$
    * degrees of freedom estimates residual errors: $n-k$ (# of data points minus # of estimated coefficients), $n>k$
    * AIS - understand this: standard errors for $\beta$ are proportional to $\sigma$

To do
finish reading chapter
do exercises
go back and clean chapter notes to only *essential* takaways
return to AIS comments




